from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import re
import time
import os

# è‡ªå®šä¹‰ä¸€ä¸ªç©ºç›®å½•è·¯å¾„ï¼ˆå»ºè®®æ”¾åœ¨éç³»ç»Ÿç›˜ï¼Œæ¯”å¦‚Dç›˜ï¼‰
crawler_profile_path = r"D:\Chrome_Crawler_Profile"
# è‡ªåŠ¨åˆ›å»ºè¯¥ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
if not os.path.exists(crawler_profile_path):
    os.makedirs(crawler_profile_path)
    print(f"å·²åˆ›å»ºç‹¬ç«‹çˆ¬è™«é…ç½®ç›®å½•ï¼š{crawler_profile_path}")

# 1. Chromeé…ç½®ï¼ˆæ ¸å¿ƒï¼šä½¿ç”¨ç‹¬ç«‹é…ç½®ç›®å½•ï¼Œéš”ç¦»ä¸»Chromeï¼‰
options = webdriver.ChromeOptions()
# æŒ‡å‘ç‹¬ç«‹é…ç½®ç›®å½•ï¼ˆå…³é”®ï¼šä¸å’Œä¸»Chromeå†²çªï¼‰
options.add_argument(f'--user-data-dir={crawler_profile_path}')
options.add_argument('--profile-directory=Default')  # è¯¥ç›®å½•ä¸‹çš„é»˜è®¤é…ç½®

# ååçˆ¬é…ç½®ï¼ˆä¸å½±å“ä¸»Chromeï¼‰
options.add_argument('--headless=new')  # æ— å¤´æ¨¡å¼
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36')
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option('useAutomationExtension', False)

# 2. å¯åŠ¨Chromeï¼ˆæ— éœ€å…³é—­ä¸»Chromeï¼‰
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    print("\nâœ… ç‹¬ç«‹Chromeçª—å£å·²å¯åŠ¨ï¼ˆä¸»Chromeå¯æ­£å¸¸ä½¿ç”¨ï¼‰")
except Exception as e:
    print(f"\nâŒ Chromeå¯åŠ¨å¤±è´¥ï¼š{e}")
    exit()

# 3. è®¿é—®çŸ¥ä¹çƒ­æœï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰
url = 'https://www.zhihu.com/hot'
driver.get(url)
driver.maximize_window()# çª—å£æœ€å¤§åŒ–

# 4. ç­‰å¾…çƒ­æœå…ƒç´ åŠ è½½
wait = WebDriverWait(driver, 15)
try:
    wait.until(EC.presence_of_element_located((By.XPATH, '//h2[contains(@class, "HotItem-title")]')))
    print("âœ… çŸ¥ä¹çƒ­æœé¡µé¢åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"\nâŒ å…ƒç´ åŠ è½½è¶…æ—¶ï¼š{e}")
    print(f"å½“å‰é¡µé¢URLï¼š{driver.current_url}")
    driver.quit()
    exit()

# 5. è§£æå¹¶æ‰“å°çƒ­æœ
page_source = driver.page_source
soup = BeautifulSoup(page_source, 'lxml')
hot_titles = soup.find_all('h2', class_=re.compile('HotItem-title'))
hot_titles2 = soup.find_all('a', herf=re.compile(''))

if hot_titles:
    print('='*60 + '\nğŸ“ˆ çŸ¥ä¹å®æ—¶çƒ­æœ\n' + '='*60)
    pattern = re.compile(r'[\u4e00-\u9fa50-9a-zA-Z%":ï¼Œã€‚ï¼ï¼Ÿã€]+')
    for idx, title_tag in enumerate(hot_titles, 1):
        raw_title = title_tag.get_text(strip=True)
        clean_title = ''.join(pattern.findall(raw_title))
        if clean_title:
            print(f'ç¬¬{idx:2d}æ¡ï¼š{clean_title}')
else:
    print("âŒ æœªæ‰¾åˆ°çƒ­æœæ ‡é¢˜ï¼ˆå¯èƒ½é¡µé¢ç»“æ„æ›´æ–°ï¼‰")

# 6. å…³é—­æµè§ˆå™¨ï¼ˆCookieå·²ä¿å­˜åœ¨ç‹¬ç«‹é…ç½®ç›®å½•ï¼Œä¸‹æ¬¡è¿è¡Œæ— éœ€ç™»å½•ï¼‰
driver.quit()
print(f"\nâœ… çˆ¬å–å®Œæˆï¼Cookieå·²ä¿å­˜åˆ°ï¼š{crawler_profile_path}")
print("âœ… ä¸‹æ¬¡è¿è¡Œæ— éœ€ç™»å½•ï¼Œä¸”ä¸å½±å“ä¸»Chromeæµè§ˆå™¨")